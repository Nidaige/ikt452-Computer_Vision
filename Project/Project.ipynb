{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e09cd77-512b-405a-9358-708e08fbb04c",
   "metadata": {},
   "source": [
    "# Project: Cel-shading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffdca0c-3b38-4402-bf67-729586f4803d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project Description\n",
    "The purpose of this project was to create an algorithm that takes in images and transforms the people in the images to a cartoony style reminiscent of the art-style in the 2009 video game \"Borderlands\" and old-school cel-shading rendering.\n",
    "The pipeline from an unprocessed image to the final result consists of a few key steps:\n",
    "    - Use semantic segmentation to only affect people\n",
    "    - Normalize brightness, or Value from HSV, into a set number of levels of brightness\n",
    "    - Use edge-detection to find the edges of each area of brightness\n",
    "    - Draw over these edges in the normalized image to outline shapes and contours in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5b57a-9ff1-4367-bc75-1e6608b19b34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Images\n",
    "Any image with a person should work, but the more clearly the person is standing out from the background, and the more differences in lighting on the person, the better.Â¨\n",
    "Here are a few example images, the first of which will be passed through each step of the algorithm so you can see what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b82897-3155-4dd0-834e-33c8bb1809cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from segmentation import blur_object_pixels\n",
    "from normalize_HSV import normalize_brightness, apply_mask, np2img\n",
    "from draw_outlines import draw_outlines\n",
    "from full_pipeline import full_pipeline\n",
    "\n",
    "# Loading the first image, to be used onwards\n",
    "filename = 'data/image6.jpg'\n",
    "input_image = Image.open(filename)\n",
    "input_image = input_image.convert(\"RGB\")\n",
    "display(input_image)\n",
    "\n",
    "# Two more example images:\n",
    "example1 = 'data/image7.jpg'\n",
    "example1 = Image.open(example1)\n",
    "display(example1)\n",
    "\n",
    "example2 = 'data/image8.jpg'\n",
    "example2 = Image.open(example2)\n",
    "display(example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1727ee24-501e-4e32-8ef5-9c50e7bf2a13",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Segmentation and Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083854b-a381-4027-b375-17e9f6bd7248",
   "metadata": {
    "tags": []
   },
   "source": [
    "The first step is to figure out where in the image a person is. This is done by using a semantic segmentation algorithm that utilizes the deeplabv3_resnet101 model from pytorch. The image is smoothed with a gaussian kernel to make the outline of the person smoother, and to improve the model's segmentation capabilities. Only the area with the person is blurred.\n",
    "\n",
    "The model predicts the locations of a wide range of objects, such as people, airplanes, bicycles, cars, buses, horses and motorbikes. The prediction for the person-object is what we're interested in, and this is readily available through a produced python dictionary.\n",
    "\n",
    "First, the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9e337-ba8c-45d4-bb6b-3ff853942021",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)\n",
    "input_image_blurred, person_mask = blur_object_pixels(model, input_image, ['person'], sigma=3, show_object_list=False, concat=False, scale=1)\n",
    "display(person_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21a19a6-c571-4d81-aa03-0971e8d3944e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normalize lighting via HSV-values\n",
    "Now that we've extracted the outline of the person, it's time to normalize the brightness into a few distinct levels.\n",
    "This is done by converting the image from the normal RGB-format over to a HSV-format, where the V - Value, represents the brightness of each pixel. By comparing the value of each pixel in the image with a range of thresholds, and setting the value to the last threshold it's brighter than, we get a variety of masks that show the different brightness areas of the image. By setting the \"high value\" of the masks to 255/N where N is the number of brightness thresholds, we can simply add the masks together to create the brightness mask of the image, as seen below.\n",
    "\n",
    "Once we have the brightness mask, we can apply it by converting the blurred image from earlier into HSV, and setting the Value of each pixel to the corresponding Value in the mask, within the confines of the segmented person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c527b3f8-0689-4bc1-8c32-83c8cd45a191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ms = normalize_brightness(image=input_image_blurred, N=4, object_mask=person_mask)  # normalize brightness into N different levels\n",
    "img = ms[\"all\"]  # get the mask with all the brightness levels composited\n",
    "display(img)  # display the full brightness mask\n",
    "masked = apply_mask(im=input_image, mask=img)  # apply the brightness mask to the image\n",
    "display(masked)  # display the normalized image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea1418-4014-42eb-b402-c703ec4eb1de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Edge Detection\n",
    "Now that we have a person with normalized brightness, we can apply an edge detection algorithm to find the outlines of the areas of similar brightness, and then draw over them to complete the cartoonish look.\n",
    "The openCV library has multiple functions for edge detection, and through testing, the best one seems to be the canny edge detection.\n",
    "\n",
    "Canny edges is a multi-step algorithm that can be read about [here](https://en.wikipedia.org/wiki/Canny_edge_detector), but the most important things is knowing the input and outputs.\n",
    "The inputs are the image itself, and two thresholds used by the algorithm to determine edges.\n",
    "\n",
    "The first image shows the edges detected in the image above, and the second image is when filtered by the segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d52904",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(masked)[:, :, ::-1].copy()  # Placeholder for the detected edges\n",
    "\n",
    "# Canny Edge Detection\n",
    "edges = Image.fromarray(cv2.Canny(image=img, threshold1=0, threshold2=200)) # Canny Edge Detection\n",
    "display(edges)\n",
    "\n",
    "# Filter detected edges by the segmented person mask\n",
    "outlines = Image.composite(edges, person_mask, person_mask)\n",
    "display(outlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe661c9c-f652-4fe3-b2bb-ee7ccc63de6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Drawing over detected edges\n",
    "Finally, all that remains is to use the detected edges to draw lines onto the normalized, segmented image. This is done simply enough by iterating over the pixels, and setting the RGB-values to 0 where the edges are detected.\n",
    "The lines can be made thicker by drawing the line in an area round the edges, but empirically the nicest looking result is when the line is only 1px wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba90894-4e88-4a09-87aa-a7c241527178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_width = 1  # width of line to draw over detected edges\n",
    "final_image = np2img(draw_outlines(masked,outlines, line_width))  # draw lines in <masked> following detected edges in <outlines>\n",
    "display(final_image)  # display the final image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cced1f",
   "metadata": {},
   "source": [
    "## Try it yourself!\n",
    "Below is all the relevant parameters and a function call that runs the entire pipeline so you can explore different results and outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'data/image6.jpg'  # path to your image\n",
    "segmentation_model ='deeplabv3_resnet101' \n",
    "blur_sigma = 3 \n",
    "processing_scale = 1 \n",
    "brightness_segments = 4 \n",
    "canny_lower_threshold = 0 \n",
    "canny_upper_threshold = 200 \n",
    "line_width = 1\n",
    "full_pipeline(image_path=image_path, segmentation_model=segmentation_model, blur_sigma=blur_sigma, processing_scale=processing_scale, brightness_segments=brightness_segments, canny_lower_threshold=canny_lower_threshold, canny_upper_threshold=canny_upper_threshold, line_width=line_width)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('computervision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8990cfdfb7eac39705f988533e56b143a8d898564f7dc071f7e40a972bb0dafd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
